{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "An Even Easier Introduction to CUDA.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2WkOA4mcN7Hj"
      },
      "source": [
        "# An Even Easier Introduction to CUDA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vuOcUi0fvogW"
      },
      "source": [
        "This notebook accompanies Mark Harris's popular blog post [_An Even Easier Introduction to CUDA_](https://developer.nvidia.com/blog/even-easier-introduction-cuda/).\n",
        "\n",
        "If you enjoy this notebook and want to learn more, the [NVIDIA DLI](https://nvidia.com/dli) offers several in depth CUDA Programming courses.\n",
        "\n",
        "For those of you just starting out, please consider [_Fundamentals of Accelerated Computing with CUDA C/C++_](https://courses.nvidia.com/courses/course-v1:DLI+C-AC-01+V1/about) which provides dedicated GPU resources, a more sophisticated programming environment, use of the [NVIDIA Nsight Systems™](https://developer.nvidia.com/nsight-systems) visual profiler, dozens of interactive exercises, detailed presentations, over 8 hours of material, and the ability to earn a DLI Certificate of Competency.\n",
        "\n",
        "Similarly, for Python programmers, please consider [_Fundamentals of Accelerated Computing with CUDA Python_](https://courses.nvidia.com/courses/course-v1:DLI+C-AC-02+V1/about).\n",
        "\n",
        "For more intermediate and advance CUDA programming materials, please check out the _Accelerated Computing_ section of the NVIDIA DLI [self-paced catalog](https://www.nvidia.com/en-us/training/online/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V1C6GK_MO5er"
      },
      "source": [
        "<img src=\"https://developer.download.nvidia.com/training/courses/T-AC-01-V1/CUDA_Cube_1K.jpeg\" width=\"400\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IcmbR8lZPLRv"
      },
      "source": [
        "This post is a super simple introduction to CUDA, the popular parallel computing platform and programming model from NVIDIA. I wrote a previous [“Easy Introduction”](https://developer.nvidia.com/blog/easy-introduction-cuda-c-and-c/) to CUDA in 2013 that has been very popular over the years. But CUDA programming has gotten easier, and GPUs have gotten much faster, so it’s time for an updated (and even easier) introduction.\n",
        "\n",
        "CUDA C++ is just one of the ways you can create massively parallel applications with CUDA. It lets you use the powerful C++ programming language to develop high performance algorithms accelerated by thousands of parallel threads running on GPUs. Many developers have accelerated their computation- and bandwidth-hungry applications this way, including the libraries and frameworks that underpin the ongoing revolution in artificial intelligence known as [Deep Learning](https://developer.nvidia.com/deep-learning).\n",
        "\n",
        "So, you’ve heard about CUDA and you are interested in learning how to use it in your own applications. If you are a C or C++ programmer, this blog post should give you a good start. To follow along, you’ll need a computer with an CUDA-capable GPU (Windows, Mac, or Linux, and any NVIDIA GPU should do), or a cloud instance with GPUs (AWS, Azure, IBM SoftLayer, and other cloud service providers have them). You’ll also need the free [CUDA Toolkit](https://developer.nvidia.com/cuda-toolkit) installed.\n",
        "\n",
        "Let's get started!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vDQ9ycz0Qfyf"
      },
      "source": [
        "<img src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2017/01/cuda_ai_cube-625x625.jpg\" width=\"400\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wH9Rfms_QtXF"
      },
      "source": [
        "## Starting Simple"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n5-iUihBQvQt"
      },
      "source": [
        "We’ll start with a simple C++ program that adds the elements of two arrays with a million elements each."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nc-gBqLDQ7AC"
      },
      "source": [
        "%%writefile add.cpp\n",
        "\n",
        "#include <iostream>\n",
        "#include <math.h>\n",
        "\n",
        "// function to add the elements of two arrays\n",
        "void add(int n, float *x, float *y)\n",
        "{\n",
        "  for (int i = 0; i < n; i++)\n",
        "      y[i] = x[i] + y[i];\n",
        "}\n",
        "\n",
        "int main(void)\n",
        "{\n",
        "  int N = 1<<20; // 1M elements\n",
        "\n",
        "  float *x = new float[N];\n",
        "  float *y = new float[N];\n",
        "\n",
        "  // initialize x and y arrays on the host\n",
        "  for (int i = 0; i < N; i++) {\n",
        "    x[i] = 1.0f;\n",
        "    y[i] = 2.0f;\n",
        "  }\n",
        "\n",
        "  // Run kernel on 1M elements on the CPU\n",
        "  add(N, x, y);\n",
        "\n",
        "  // Check for errors (all values should be 3.0f)\n",
        "  float maxError = 0.0f;\n",
        "  for (int i = 0; i < N; i++)\n",
        "    maxError = fmax(maxError, fabs(y[i]-3.0f));\n",
        "  std::cout << \"Max error: \" << maxError << std::endl;\n",
        "\n",
        "  // Free memory\n",
        "  delete [] x;\n",
        "  delete [] y;\n",
        "\n",
        "  return 0;\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gw6DsX4uRHMg"
      },
      "source": [
        "Executing the above cell will save its contents to the file add.cpp.\n",
        "\n",
        "The following cell will compile and run this C++ program."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gNpH54M_RbAU"
      },
      "source": [
        "%%shell\n",
        "g++ add.cpp -o add"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6V2tGPYRi3l"
      },
      "source": [
        "Then run it:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QmA4ACe5RuiU"
      },
      "source": [
        "%%shell\n",
        "./add"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9IAWYlniR153"
      },
      "source": [
        "As expected, it prints that there was no error in the summation and then exits. Now I want to get this computation running (in parallel) on the many cores of a GPU. It’s actually pretty easy to take the first steps.\n",
        "\n",
        "First, I just have to turn our `add` function into a function that the GPU can run, called a *kernel* in CUDA. To do this, all I have to do is add the specifier `__global__` to the function, which tells the CUDA C++ compiler that this is a function that runs on the GPU and can be called from CPU code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "heY-lpzjSHfB"
      },
      "source": [
        "```cpp\n",
        "// CUDA Kernel function to add the elements of two arrays on the GPU\n",
        "__global__\n",
        "void add(int n, float *x, float *y)\n",
        "{\n",
        "  for (int i = 0; i < n; i++)\n",
        "      y[i] = x[i] + y[i];\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kozMbHdpSKNu"
      },
      "source": [
        "These `__global__` functions are known as *kernels*, and code that runs on the GPU is often called *device code*, while code that runs on the CPU is *host code*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VhnBGGU-SWiN"
      },
      "source": [
        "## Memory Allocation in CUDA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RvIDRBk2SbqA"
      },
      "source": [
        "To compute on the GPU, I need to allocate memory accessible by the GPU. [Unified Memory](https://developer.nvidia.com/blog/unified-memory-in-cuda-6/) in CUDA makes this easy by providing a single memory space accessible by all GPUs and CPUs in your system. To allocate data in unified memory, call `cudaMallocManaged()`, which returns a pointer that you can access from host (CPU) code or device (GPU) code. To free the data, just pass the pointer to `cudaFree()`.\n",
        "\n",
        "I just need to replace the calls to `new` in the code above with calls to `cudaMallocManaged()`, and replace calls to `delete []` with calls to `cudaFree`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IxCut_urS46H"
      },
      "source": [
        "```cpp\n",
        "  // Allocate Unified Memory -- accessible from CPU or GPU\n",
        "  float *x, *y;\n",
        "  cudaMallocManaged(&x, N*sizeof(float));\n",
        "  cudaMallocManaged(&y, N*sizeof(float));\n",
        "\n",
        "  ...\n",
        "\n",
        "  // Free memory\n",
        "  cudaFree(x);\n",
        "  cudaFree(y);\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2oEf2B-1S-1V"
      },
      "source": [
        "Finally, I need to *launch* the `add()` kernel, which invokes it on the GPU. CUDA kernel launches are specified using the triple angle bracket syntax `<<< >>>`. I just have to add it to the call to `add` before the parameter list."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bqTJlvWLS7iW"
      },
      "source": [
        "```cpp\n",
        "add<<<1, 1>>>(N, x, y);\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGf0ZiTOTTHU"
      },
      "source": [
        "Easy! I’ll get into the details of what goes inside the angle brackets soon; for now all you need to know is that this line launches one GPU thread to run `add()`.\n",
        "\n",
        "Just one more thing: I need the CPU to wait until the kernel is done before it accesses the results (because CUDA kernel launches don’t block the calling CPU thread). To do this I just call `cudaDeviceSynchronize()` before doing the final error checking on the CPU.\n",
        "\n",
        "Here’s the complete code:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K8bYDM7kYT7S"
      },
      "source": [
        "%%writefile add.cu\n",
        "\n",
        "#include <iostream>\n",
        "#include <math.h>\n",
        "// Kernel function to add the elements of two arrays\n",
        "__global__\n",
        "void add(int n, float *x, float *y)\n",
        "{\n",
        "  for (int i = 0; i < n; i++)\n",
        "    y[i] = x[i] + y[i];\n",
        "}\n",
        "\n",
        "int main(void)\n",
        "{\n",
        "  int N = 1<<20\n",
        " ;\n",
        "  float *x, *y;\n",
        "\n",
        "  // Allocate Unified Memory – accessible from CPU or GPU\n",
        "  cudaMallocManaged(&x, N*sizeof(float));\n",
        "  cudaMallocManaged(&y, N*sizeof(float));\n",
        "\n",
        "  // initialize x and y arrays on the host\n",
        "  for (int i = 0; i < N; i++) {\n",
        "    x[i] = 1.0f;\n",
        "    y[i] = 2.0f;\n",
        "  }\n",
        "\n",
        "  // Run kernel on 1M elements on the GPU\n",
        "  add<<<1, 1>>>(N, x, y);\n",
        "\n",
        "  // Wait for GPU to finish before accessing on host\n",
        "  cudaDeviceSynchronize();\n",
        "\n",
        "  // Check for errors (all values should be 3.0f)\n",
        "  float maxError = 0.0f;\n",
        "  for (int i = 0; i < N; i++)\n",
        "    maxError = fmax(maxError, fabs(y[i]-3.0f));\n",
        "  std::cout << \"Max error: \" << maxError << std::endl;\n",
        "\n",
        "  // Free memory\n",
        "  cudaFree(x);\n",
        "  cudaFree(y);\n",
        "\n",
        "  return 0;\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TjLGGp0oYeEc"
      },
      "source": [
        "%%shell\n",
        "\n",
        "nvcc add.cu -o add_cuda\n",
        "./add_cuda"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ATssEzEYqGx"
      },
      "source": [
        "This is only a first step, because as written, this kernel is only correct for a single thread, since every thread that runs it will perform the add on the whole array. Moreover, there is a [race condition](https://en.wikipedia.org/wiki/Race_condition) since multiple parallel threads would both read and write the same locations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3kKpDoZ-YzJ8"
      },
      "source": [
        "## Profile it!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-BC-CWVZglt"
      },
      "source": [
        "I think the simplest way to find out how long the kernel takes to run is to run it with `nvprof`, the command line GPU profiler that comes with the CUDA Toolkit. Just type `nvprof ./add_cuda` on the command line:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gtfQLWwYZpfV"
      },
      "source": [
        "%%shell\n",
        "\n",
        "nvprof ./add_cuda"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F9Dn4ZV-Z_UJ"
      },
      "source": [
        "The above will show the single call to `add`. Your timing may vary depending on the GPU allocated to you by Colab. To see the current GPU allocated to you run the following cell and look in the `Name` column where you might see, for example `Tesla T4`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TrYmwVZfaPqz"
      },
      "source": [
        "%%shell\n",
        "\n",
        "nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-MWYteAVadCs"
      },
      "source": [
        "Let's make it faster with parallelism."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SaiMC73Falvb"
      },
      "source": [
        "## Picking up the Threads"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KDFuBr_2apuJ"
      },
      "source": [
        "Now that you’ve run a kernel with one thread that does some computation, how do you make it parallel? The key is in CUDA’s `<<<1, 1>>>` syntax. This is called the execution configuration, and it tells the CUDA runtime how many parallel threads to use for the launch on the GPU. There are two parameters here, but let’s start by changing the second one: the number of threads in a thread block. CUDA GPUs run kernels using blocks of threads that are a multiple of 32 in size, so 256 threads is a reasonable size to choose."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2Pmyj0KavgB"
      },
      "source": [
        "```cpp\n",
        "add<<<1, 256>>>(N, x, y);\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oAYpH9Ctay5G"
      },
      "source": [
        "If I run the code with only this change, it will do the computation once per thread, rather than spreading the computation across the parallel threads. To do it properly, I need to modify the kernel. CUDA C++ provides keywords that let kernels get the indices of the running threads. Specifically, `threadIdx.x` contains the index of the current thread within its block, and `blockDim.x` contains the number of threads in the block. I’ll just modify the loop to stride through the array with parallel threads."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TSiqhFK_a6N3"
      },
      "source": [
        "```cpp\n",
        "__global__\n",
        "void add(int n, float *x, float *y)\n",
        "{\n",
        "  int index = threadIdx.x;\n",
        "  int stride = blockDim.x;\n",
        "  for (int i = index; i < n; i += stride)\n",
        "      y[i] = x[i] + y[i];\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_7mYcBzOa9zR"
      },
      "source": [
        "The `add` function hasn’t changed that much. In fact, setting `index` to 0 and `stride` to 1 makes it semantically identical to the first version.\n",
        "\n",
        "Here we save the file as add_block.cu and compile and run it in `nvprof` again."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "goCKY9QNbPZ-"
      },
      "source": [
        "%%writefile add_block.cu\n",
        "\n",
        "#include <iostream>\n",
        "#include <math.h>\n",
        "\n",
        "// Kernel function to add the elements of two arrays\n",
        "__global__\n",
        "void add(int n, float *x, float *y)\n",
        "{\n",
        "  int index = threadIdx.x;\n",
        "  int stride = blockDim.x;\n",
        "  for (int i = index; i < n; i += stride)\n",
        "      y[i] = x[i] + y[i];\n",
        "}\n",
        "\n",
        "int main(void)\n",
        "{\n",
        "  int N = 1<<20;\n",
        "  float *x, *y;\n",
        "\n",
        "  // Allocate Unified Memory – accessible from CPU or GPU\n",
        "  cudaMallocManaged(&x, N*sizeof(float));\n",
        "  cudaMallocManaged(&y, N*sizeof(float));\n",
        "\n",
        "  // initialize x and y arrays on the host\n",
        "  for (int i = 0; i < N; i++) {\n",
        "    x[i] = 1.0f;\n",
        "    y[i] = 2.0f;\n",
        "  }\n",
        "\n",
        "  // Run kernel on 1M elements on the GPU\n",
        "  add<<<1, 256>>>(N, x, y);\n",
        "\n",
        "  // Wait for GPU to finish before accessing on host\n",
        "  cudaDeviceSynchronize();\n",
        "\n",
        "  // Check for errors (all values should be 3.0f)\n",
        "  float maxError = 0.0f;\n",
        "  for (int i = 0; i < N; i++)\n",
        "    maxError = fmax(maxError, fabs(y[i]-3.0f));\n",
        "  std::cout << \"Max error: \" << maxError << std::endl;\n",
        "\n",
        "  // Free memory\n",
        "  cudaFree(x);\n",
        "  cudaFree(y);\n",
        "\n",
        "  return 0;\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l9cmfbcVbYgD"
      },
      "source": [
        "%%shell\n",
        "\n",
        "nvcc add_block.cu -o add_block\n",
        "nvprof ./add_block"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fo5KaV3Nba7g"
      },
      "source": [
        "That’s a big speedup (compare the time for the `add` kernel by looking at the `GPU activities` field), but not surprising since I went from 1 thread to 256 threads. Let’s keep going to get even more performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YtgQWOyMcPfn"
      },
      "source": [
        "## Out of the Blocks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wAoFGwmbcRbN"
      },
      "source": [
        "CUDA GPUs have many parallel processors grouped into Streaming Multiprocessors, or SMs. Each SM can run multiple concurrent thread blocks. As an example, a Tesla P100 GPU based on the [Pascal GPU Architecture](https://developer.nvidia.com/blog/inside-pascal/) has 56 SMs, each capable of supporting up to 2048 active threads. To take full advantage of all these threads, I should launch the kernel with multiple thread blocks.\n",
        "\n",
        "By now you may have guessed that the first parameter of the execution configuration specifies the number of thread blocks. Together, the blocks of parallel threads make up what is known as the *grid*. Since I have `N` elements to process, and 256 threads per block, I just need to calculate the number of blocks to get at least `N` threads. I simply divide `N` by the block size (being careful to round up in case `N` is not a multiple of `blockSize`)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AnI2II2ockgC"
      },
      "source": [
        "```cpp\n",
        "int blockSize = 256;\n",
        "int numBlocks = (N + blockSize - 1) / blockSize;\n",
        "add<<<numBlocks, blockSize>>>(N, x, y);\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ayq2MJZLctY0"
      },
      "source": [
        "<img src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2017/01/cuda_indexing.png\" width=\"800\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZduP7RWc3Je"
      },
      "source": [
        "I also need to update the kernel code to take into account the entire grid of thread blocks. CUDA provides `gridDim.x`, which contains the number of blocks in the grid, and `blockIdx.x`, which contains the index of the current thread block in the grid. Figure 1 illustrates the the approach to indexing into an array (one-dimensional) in CUDA using `blockDim.x`, `gridDim.x`, and `threadIdx.x`. The idea is that each thread gets its index by computing the offset to the beginning of its block (the block index times the block size: `blockIdx.x * blockDim.x`) and adding the thread’s index within the block (`threadIdx.x`). The code `blockIdx.x * blockDim.x + threadIdx.x` is idiomatic CUDA."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6cI2WLEAeG5y"
      },
      "source": [
        "```cpp\n",
        "__global__\n",
        "void add(int n, float *x, float *y)\n",
        "{\n",
        "  int index = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "  int stride = blockDim.x * gridDim.x;\n",
        "  for (int i = index; i < n; i += stride)\n",
        "    y[i] = x[i] + y[i];\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83hC-rCLdPHC"
      },
      "source": [
        "The updated kernel also sets stride to the total number of threads in the grid (`blockDim.x * gridDim.x`). This type of loop in a CUDA kernel is often called a [*grid-stride*](https://developer.nvidia.com/blog/cuda-pro-tip-write-flexible-kernels-grid-stride-loops/) loop.\n",
        "\n",
        "Save the file as `add_grid.cu` and compile and run it in `nvprof` again."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a7w-DHBRdhUC",
        "outputId": "36638e47-133f-4441-8442-2375dbfd9b10",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%%writefile add_grid.cu\n",
        "\n",
        "#include <iostream>\n",
        "#include <math.h>\n",
        "\n",
        "// Kernel function to add the elements of two arrays\n",
        "__global__\n",
        "void add(int n, float *x, float *y)\n",
        "{\n",
        "  int index = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "  int stride = blockDim.x * gridDim.x;\n",
        "\n",
        "  if (threadIdx.x < 8) {\n",
        "    printf(\"b=%d t=%d index=%d stride=%d\\n\", blockIdx.x, threadIdx.x, index, stride);\n",
        "  }\n",
        "\n",
        "  for (int i = index; i < n; i += stride)\n",
        "    y[i] = x[i] + y[i];\n",
        "}\n",
        "\n",
        "int main(void)\n",
        "{\n",
        "  int N = 1<<20;\n",
        "  float *x, *y;\n",
        "\n",
        "  // Allocate Unified Memory – accessible from CPU or GPU\n",
        "  cudaMallocManaged(&x, N*sizeof(float));\n",
        "  cudaMallocManaged(&y, N*sizeof(float));\n",
        "\n",
        "  // initialize x and y arrays on the host\n",
        "  for (int i = 0; i < N; i++) {\n",
        "    x[i] = 1.0f;\n",
        "    y[i] = 2.0f;\n",
        "  }\n",
        "\n",
        "  // Run kernel on 1M elements on the GPU\n",
        "  int blockSize = 256;\n",
        "  int numBlocks = (N + blockSize - 1) / blockSize;\n",
        "  add<<<numBlocks, blockSize>>>(N, x, y);\n",
        "\n",
        "  // Wait for GPU to finish before accessing on host\n",
        "  cudaDeviceSynchronize();\n",
        "\n",
        "  // Check for errors (all values should be 3.0f)\n",
        "  float maxError = 0.0f;\n",
        "  for (int i = 0; i < N; i++)\n",
        "    maxError = fmax(maxError, fabs(y[i]-3.0f));\n",
        "  std::cout << \"Max error: \" << maxError << std::endl;\n",
        "\n",
        "  // Free memory\n",
        "  cudaFree(x);\n",
        "  cudaFree(y);\n",
        "\n",
        "  return 0;\n",
        "}"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting add_grid.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FhcrktW9dw34",
        "outputId": "43e14062-93a1-4134-cde4-3872c811b7d7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%%shell\n",
        "\n",
        "nvcc -arch=sm_70 add_grid.cu -o add_grid\n",
        "./add_grid"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b=822 t=0 index=210432 stride=1048576\n",
            "b=822 t=1 index=210433 stride=1048576\n",
            "b=822 t=2 index=210434 stride=1048576\n",
            "b=822 t=3 index=210435 stride=1048576\n",
            "b=822 t=4 index=210436 stride=1048576\n",
            "b=822 t=5 index=210437 stride=1048576\n",
            "b=822 t=6 index=210438 stride=1048576\n",
            "b=822 t=7 index=210439 stride=1048576\n",
            "b=830 t=0 index=212480 stride=1048576\n",
            "b=830 t=1 index=212481 stride=1048576\n",
            "b=830 t=2 index=212482 stride=1048576\n",
            "b=830 t=3 index=212483 stride=1048576\n",
            "b=830 t=4 index=212484 stride=1048576\n",
            "b=830 t=5 index=212485 stride=1048576\n",
            "b=830 t=6 index=212486 stride=1048576\n",
            "b=830 t=7 index=212487 stride=1048576\n",
            "b=810 t=0 index=207360 stride=1048576\n",
            "b=810 t=1 index=207361 stride=1048576\n",
            "b=810 t=2 index=207362 stride=1048576\n",
            "b=810 t=3 index=207363 stride=1048576\n",
            "b=810 t=4 index=207364 stride=1048576\n",
            "b=810 t=5 index=207365 stride=1048576\n",
            "b=810 t=6 index=207366 stride=1048576\n",
            "b=810 t=7 index=207367 stride=1048576\n",
            "b=813 t=0 index=208128 stride=1048576\n",
            "b=813 t=1 index=208129 stride=1048576\n",
            "b=813 t=2 index=208130 stride=1048576\n",
            "b=813 t=3 index=208131 stride=1048576\n",
            "b=813 t=4 index=208132 stride=1048576\n",
            "b=813 t=5 index=208133 stride=1048576\n",
            "b=813 t=6 index=208134 stride=1048576\n",
            "b=813 t=7 index=208135 stride=1048576\n",
            "b=808 t=0 index=206848 stride=1048576\n",
            "b=808 t=1 index=206849 stride=1048576\n",
            "b=808 t=2 index=206850 stride=1048576\n",
            "b=808 t=3 index=206851 stride=1048576\n",
            "b=808 t=4 index=206852 stride=1048576\n",
            "b=808 t=5 index=206853 stride=1048576\n",
            "b=808 t=6 index=206854 stride=1048576\n",
            "b=808 t=7 index=206855 stride=1048576\n",
            "b=811 t=0 index=207616 stride=1048576\n",
            "b=811 t=1 index=207617 stride=1048576\n",
            "b=811 t=2 index=207618 stride=1048576\n",
            "b=811 t=3 index=207619 stride=1048576\n",
            "b=811 t=4 index=207620 stride=1048576\n",
            "b=811 t=5 index=207621 stride=1048576\n",
            "b=811 t=6 index=207622 stride=1048576\n",
            "b=811 t=7 index=207623 stride=1048576\n",
            "b=815 t=0 index=208640 stride=1048576\n",
            "b=815 t=1 index=208641 stride=1048576\n",
            "b=815 t=2 index=208642 stride=1048576\n",
            "b=815 t=3 index=208643 stride=1048576\n",
            "b=815 t=4 index=208644 stride=1048576\n",
            "b=815 t=5 index=208645 stride=1048576\n",
            "b=815 t=6 index=208646 stride=1048576\n",
            "b=815 t=7 index=208647 stride=1048576\n",
            "b=820 t=0 index=209920 stride=1048576\n",
            "b=820 t=1 index=209921 stride=1048576\n",
            "b=820 t=2 index=209922 stride=1048576\n",
            "b=820 t=3 index=209923 stride=1048576\n",
            "b=820 t=4 index=209924 stride=1048576\n",
            "b=820 t=5 index=209925 stride=1048576\n",
            "b=820 t=6 index=209926 stride=1048576\n",
            "b=820 t=7 index=209927 stride=1048576\n",
            "b=843 t=0 index=215808 stride=1048576\n",
            "b=843 t=1 index=215809 stride=1048576\n",
            "b=843 t=2 index=215810 stride=1048576\n",
            "b=843 t=3 index=215811 stride=1048576\n",
            "b=843 t=4 index=215812 stride=1048576\n",
            "b=843 t=5 index=215813 stride=1048576\n",
            "b=843 t=6 index=215814 stride=1048576\n",
            "b=843 t=7 index=215815 stride=1048576\n",
            "b=809 t=0 index=207104 stride=1048576\n",
            "b=809 t=1 index=207105 stride=1048576\n",
            "b=809 t=2 index=207106 stride=1048576\n",
            "b=809 t=3 index=207107 stride=1048576\n",
            "b=809 t=4 index=207108 stride=1048576\n",
            "b=809 t=5 index=207109 stride=1048576\n",
            "b=809 t=6 index=207110 stride=1048576\n",
            "b=809 t=7 index=207111 stride=1048576\n",
            "b=823 t=0 index=210688 stride=1048576\n",
            "b=823 t=1 index=210689 stride=1048576\n",
            "b=823 t=2 index=210690 stride=1048576\n",
            "b=823 t=3 index=210691 stride=1048576\n",
            "b=823 t=4 index=210692 stride=1048576\n",
            "b=823 t=5 index=210693 stride=1048576\n",
            "b=823 t=6 index=210694 stride=1048576\n",
            "b=823 t=7 index=210695 stride=1048576\n",
            "b=814 t=0 index=208384 stride=1048576\n",
            "b=814 t=1 index=208385 stride=1048576\n",
            "b=814 t=2 index=208386 stride=1048576\n",
            "b=814 t=3 index=208387 stride=1048576\n",
            "b=814 t=4 index=208388 stride=1048576\n",
            "b=814 t=5 index=208389 stride=1048576\n",
            "b=814 t=6 index=208390 stride=1048576\n",
            "b=814 t=7 index=208391 stride=1048576\n",
            "b=832 t=0 index=212992 stride=1048576\n",
            "b=832 t=1 index=212993 stride=1048576\n",
            "b=832 t=2 index=212994 stride=1048576\n",
            "b=832 t=3 index=212995 stride=1048576\n",
            "b=832 t=4 index=212996 stride=1048576\n",
            "b=832 t=5 index=212997 stride=1048576\n",
            "b=832 t=6 index=212998 stride=1048576\n",
            "b=832 t=7 index=212999 stride=1048576\n",
            "b=812 t=0 index=207872 stride=1048576\n",
            "b=812 t=1 index=207873 stride=1048576\n",
            "b=812 t=2 index=207874 stride=1048576\n",
            "b=812 t=3 index=207875 stride=1048576\n",
            "b=812 t=4 index=207876 stride=1048576\n",
            "b=812 t=5 index=207877 stride=1048576\n",
            "b=812 t=6 index=207878 stride=1048576\n",
            "b=812 t=7 index=207879 stride=1048576\n",
            "b=828 t=0 index=211968 stride=1048576\n",
            "b=828 t=1 index=211969 stride=1048576\n",
            "b=828 t=2 index=211970 stride=1048576\n",
            "b=828 t=3 index=211971 stride=1048576\n",
            "b=828 t=4 index=211972 stride=1048576\n",
            "b=828 t=5 index=211973 stride=1048576\n",
            "b=828 t=6 index=211974 stride=1048576\n",
            "b=828 t=7 index=211975 stride=1048576\n",
            "b=818 t=0 index=209408 stride=1048576\n",
            "b=818 t=1 index=209409 stride=1048576\n",
            "b=818 t=2 index=209410 stride=1048576\n",
            "b=818 t=3 index=209411 stride=1048576\n",
            "b=818 t=4 index=209412 stride=1048576\n",
            "b=818 t=5 index=209413 stride=1048576\n",
            "b=818 t=6 index=209414 stride=1048576\n",
            "b=818 t=7 index=209415 stride=1048576\n",
            "b=821 t=0 index=210176 stride=1048576\n",
            "b=821 t=1 index=210177 stride=1048576\n",
            "b=821 t=2 index=210178 stride=1048576\n",
            "b=821 t=3 index=210179 stride=1048576\n",
            "b=821 t=4 index=210180 stride=1048576\n",
            "b=821 t=5 index=210181 stride=1048576\n",
            "b=821 t=6 index=210182 stride=1048576\n",
            "b=821 t=7 index=210183 stride=1048576\n",
            "b=835 t=0 index=213760 stride=1048576\n",
            "b=835 t=1 index=213761 stride=1048576\n",
            "b=835 t=2 index=213762 stride=1048576\n",
            "b=835 t=3 index=213763 stride=1048576\n",
            "b=835 t=4 index=213764 stride=1048576\n",
            "b=835 t=5 index=213765 stride=1048576\n",
            "b=835 t=6 index=213766 stride=1048576\n",
            "b=835 t=7 index=213767 stride=1048576\n",
            "b=827 t=0 index=211712 stride=1048576\n",
            "b=827 t=1 index=211713 stride=1048576\n",
            "b=827 t=2 index=211714 stride=1048576\n",
            "b=827 t=3 index=211715 stride=1048576\n",
            "b=827 t=4 index=211716 stride=1048576\n",
            "b=827 t=5 index=211717 stride=1048576\n",
            "b=827 t=6 index=211718 stride=1048576\n",
            "b=827 t=7 index=211719 stride=1048576\n",
            "b=833 t=0 index=213248 stride=1048576\n",
            "b=833 t=1 index=213249 stride=1048576\n",
            "b=833 t=2 index=213250 stride=1048576\n",
            "b=833 t=3 index=213251 stride=1048576\n",
            "b=833 t=4 index=213252 stride=1048576\n",
            "b=833 t=5 index=213253 stride=1048576\n",
            "b=833 t=6 index=213254 stride=1048576\n",
            "b=833 t=7 index=213255 stride=1048576\n",
            "b=831 t=0 index=212736 stride=1048576\n",
            "b=831 t=1 index=212737 stride=1048576\n",
            "b=831 t=2 index=212738 stride=1048576\n",
            "b=831 t=3 index=212739 stride=1048576\n",
            "b=831 t=4 index=212740 stride=1048576\n",
            "b=831 t=5 index=212741 stride=1048576\n",
            "b=831 t=6 index=212742 stride=1048576\n",
            "b=831 t=7 index=212743 stride=1048576\n",
            "b=834 t=0 index=213504 stride=1048576\n",
            "b=834 t=1 index=213505 stride=1048576\n",
            "b=834 t=2 index=213506 stride=1048576\n",
            "b=834 t=3 index=213507 stride=1048576\n",
            "b=834 t=4 index=213508 stride=1048576\n",
            "b=834 t=5 index=213509 stride=1048576\n",
            "b=834 t=6 index=213510 stride=1048576\n",
            "b=834 t=7 index=213511 stride=1048576\n",
            "b=838 t=0 index=214528 stride=1048576\n",
            "b=838 t=1 index=214529 stride=1048576\n",
            "b=838 t=2 index=214530 stride=1048576\n",
            "b=838 t=3 index=214531 stride=1048576\n",
            "b=838 t=4 index=214532 stride=1048576\n",
            "b=838 t=5 index=214533 stride=1048576\n",
            "b=838 t=6 index=214534 stride=1048576\n",
            "b=838 t=7 index=214535 stride=1048576\n",
            "b=826 t=0 index=211456 stride=1048576\n",
            "b=826 t=1 index=211457 stride=1048576\n",
            "b=826 t=2 index=211458 stride=1048576\n",
            "b=826 t=3 index=211459 stride=1048576\n",
            "b=826 t=4 index=211460 stride=1048576\n",
            "b=826 t=5 index=211461 stride=1048576\n",
            "b=826 t=6 index=211462 stride=1048576\n",
            "b=826 t=7 index=211463 stride=1048576\n",
            "b=842 t=0 index=215552 stride=1048576\n",
            "b=842 t=1 index=215553 stride=1048576\n",
            "b=842 t=2 index=215554 stride=1048576\n",
            "b=842 t=3 index=215555 stride=1048576\n",
            "b=842 t=4 index=215556 stride=1048576\n",
            "b=842 t=5 index=215557 stride=1048576\n",
            "b=842 t=6 index=215558 stride=1048576\n",
            "b=842 t=7 index=215559 stride=1048576\n",
            "b=829 t=0 index=212224 stride=1048576\n",
            "b=829 t=1 index=212225 stride=1048576\n",
            "b=829 t=2 index=212226 stride=1048576\n",
            "b=829 t=3 index=212227 stride=1048576\n",
            "b=829 t=4 index=212228 stride=1048576\n",
            "b=829 t=5 index=212229 stride=1048576\n",
            "b=829 t=6 index=212230 stride=1048576\n",
            "b=829 t=7 index=212231 stride=1048576\n",
            "b=841 t=0 index=215296 stride=1048576\n",
            "b=841 t=1 index=215297 stride=1048576\n",
            "b=841 t=2 index=215298 stride=1048576\n",
            "b=841 t=3 index=215299 stride=1048576\n",
            "b=841 t=4 index=215300 stride=1048576\n",
            "b=841 t=5 index=215301 stride=1048576\n",
            "b=841 t=6 index=215302 stride=1048576\n",
            "b=841 t=7 index=215303 stride=1048576\n",
            "b=844 t=0 index=216064 stride=1048576\n",
            "b=844 t=1 index=216065 stride=1048576\n",
            "b=844 t=2 index=216066 stride=1048576\n",
            "b=844 t=3 index=216067 stride=1048576\n",
            "b=844 t=4 index=216068 stride=1048576\n",
            "b=844 t=5 index=216069 stride=1048576\n",
            "b=844 t=6 index=216070 stride=1048576\n",
            "b=844 t=7 index=216071 stride=1048576\n",
            "b=816 t=0 index=208896 stride=1048576\n",
            "b=816 t=1 index=208897 stride=1048576\n",
            "b=816 t=2 index=208898 stride=1048576\n",
            "b=816 t=3 index=208899 stride=1048576\n",
            "b=816 t=4 index=208900 stride=1048576\n",
            "b=816 t=5 index=208901 stride=1048576\n",
            "b=816 t=6 index=208902 stride=1048576\n",
            "b=816 t=7 index=208903 stride=1048576\n",
            "b=846 t=0 index=216576 stride=1048576\n",
            "b=846 t=1 index=216577 stride=1048576\n",
            "b=846 t=2 index=216578 stride=1048576\n",
            "b=846 t=3 index=216579 stride=1048576\n",
            "b=846 t=4 index=216580 stride=1048576\n",
            "b=846 t=5 index=216581 stride=1048576\n",
            "b=846 t=6 index=216582 stride=1048576\n",
            "b=846 t=7 index=216583 stride=1048576\n",
            "b=845 t=0 index=216320 stride=1048576\n",
            "b=845 t=1 index=216321 stride=1048576\n",
            "b=845 t=2 index=216322 stride=1048576\n",
            "b=845 t=3 index=216323 stride=1048576\n",
            "b=845 t=4 index=216324 stride=1048576\n",
            "b=845 t=5 index=216325 stride=1048576\n",
            "b=845 t=6 index=216326 stride=1048576\n",
            "b=845 t=7 index=216327 stride=1048576\n",
            "b=817 t=0 index=209152 stride=1048576\n",
            "b=817 t=1 index=209153 stride=1048576\n",
            "b=817 t=2 index=209154 stride=1048576\n",
            "b=817 t=3 index=209155 stride=1048576\n",
            "b=817 t=4 index=209156 stride=1048576\n",
            "b=817 t=5 index=209157 stride=1048576\n",
            "b=817 t=6 index=209158 stride=1048576\n",
            "b=817 t=7 index=209159 stride=1048576\n",
            "b=840 t=0 index=215040 stride=1048576\n",
            "b=840 t=1 index=215041 stride=1048576\n",
            "b=840 t=2 index=215042 stride=1048576\n",
            "b=840 t=3 index=215043 stride=1048576\n",
            "b=840 t=4 index=215044 stride=1048576\n",
            "b=840 t=5 index=215045 stride=1048576\n",
            "b=840 t=6 index=215046 stride=1048576\n",
            "b=840 t=7 index=215047 stride=1048576\n",
            "b=825 t=0 index=211200 stride=1048576\n",
            "b=825 t=1 index=211201 stride=1048576\n",
            "b=825 t=2 index=211202 stride=1048576\n",
            "b=825 t=3 index=211203 stride=1048576\n",
            "b=825 t=4 index=211204 stride=1048576\n",
            "b=825 t=5 index=211205 stride=1048576\n",
            "b=825 t=6 index=211206 stride=1048576\n",
            "b=825 t=7 index=211207 stride=1048576\n",
            "b=819 t=0 index=209664 stride=1048576\n",
            "b=819 t=1 index=209665 stride=1048576\n",
            "b=819 t=2 index=209666 stride=1048576\n",
            "b=819 t=3 index=209667 stride=1048576\n",
            "b=819 t=4 index=209668 stride=1048576\n",
            "b=819 t=5 index=209669 stride=1048576\n",
            "b=819 t=6 index=209670 stride=1048576\n",
            "b=819 t=7 index=209671 stride=1048576\n",
            "b=836 t=0 index=214016 stride=1048576\n",
            "b=836 t=1 index=214017 stride=1048576\n",
            "b=836 t=2 index=214018 stride=1048576\n",
            "b=836 t=3 index=214019 stride=1048576\n",
            "b=836 t=4 index=214020 stride=1048576\n",
            "b=836 t=5 index=214021 stride=1048576\n",
            "b=836 t=6 index=214022 stride=1048576\n",
            "b=836 t=7 index=214023 stride=1048576\n",
            "b=824 t=0 index=210944 stride=1048576\n",
            "b=824 t=1 index=210945 stride=1048576\n",
            "b=824 t=2 index=210946 stride=1048576\n",
            "b=824 t=3 index=210947 stride=1048576\n",
            "b=824 t=4 index=210948 stride=1048576\n",
            "b=824 t=5 index=210949 stride=1048576\n",
            "b=824 t=6 index=210950 stride=1048576\n",
            "b=824 t=7 index=210951 stride=1048576\n",
            "b=837 t=0 index=214272 stride=1048576\n",
            "b=837 t=1 index=214273 stride=1048576\n",
            "b=837 t=2 index=214274 stride=1048576\n",
            "b=837 t=3 index=214275 stride=1048576\n",
            "b=837 t=4 index=214276 stride=1048576\n",
            "b=837 t=5 index=214277 stride=1048576\n",
            "b=837 t=6 index=214278 stride=1048576\n",
            "b=837 t=7 index=214279 stride=1048576\n",
            "b=847 t=0 index=216832 stride=1048576\n",
            "b=847 t=1 index=216833 stride=1048576\n",
            "b=847 t=2 index=216834 stride=1048576\n",
            "b=847 t=3 index=216835 stride=1048576\n",
            "b=847 t=4 index=216836 stride=1048576\n",
            "b=847 t=5 index=216837 stride=1048576\n",
            "b=847 t=6 index=216838 stride=1048576\n",
            "b=847 t=7 index=216839 stride=1048576\n",
            "b=839 t=0 index=214784 stride=1048576\n",
            "b=839 t=1 index=214785 stride=1048576\n",
            "b=839 t=2 index=214786 stride=1048576\n",
            "b=839 t=3 index=214787 stride=1048576\n",
            "b=839 t=4 index=214788 stride=1048576\n",
            "b=839 t=5 index=214789 stride=1048576\n",
            "b=839 t=6 index=214790 stride=1048576\n",
            "b=839 t=7 index=214791 stride=1048576\n",
            "b=852 t=0 index=218112 stride=1048576\n",
            "b=852 t=1 index=218113 stride=1048576\n",
            "b=852 t=2 index=218114 stride=1048576\n",
            "b=852 t=3 index=218115 stride=1048576\n",
            "b=852 t=4 index=218116 stride=1048576\n",
            "b=852 t=5 index=218117 stride=1048576\n",
            "b=852 t=6 index=218118 stride=1048576\n",
            "b=852 t=7 index=218119 stride=1048576\n",
            "b=856 t=0 index=219136 stride=1048576\n",
            "b=856 t=1 index=219137 stride=1048576\n",
            "b=856 t=2 index=219138 stride=1048576\n",
            "b=856 t=3 index=219139 stride=1048576\n",
            "b=856 t=4 index=219140 stride=1048576\n",
            "b=856 t=5 index=219141 stride=1048576\n",
            "b=856 t=6 index=219142 stride=1048576\n",
            "b=856 t=7 index=219143 stride=1048576\n",
            "b=853 t=0 index=218368 stride=1048576\n",
            "b=853 t=1 index=218369 stride=1048576\n",
            "b=853 t=2 index=218370 stride=1048576\n",
            "b=853 t=3 index=218371 stride=1048576\n",
            "b=853 t=4 index=218372 stride=1048576\n",
            "b=853 t=5 index=218373 stride=1048576\n",
            "b=853 t=6 index=218374 stride=1048576\n",
            "b=853 t=7 index=218375 stride=1048576\n",
            "b=869 t=0 index=222464 stride=1048576\n",
            "b=869 t=1 index=222465 stride=1048576\n",
            "b=869 t=2 index=222466 stride=1048576\n",
            "b=869 t=3 index=222467 stride=1048576\n",
            "b=869 t=4 index=222468 stride=1048576\n",
            "b=869 t=5 index=222469 stride=1048576\n",
            "b=869 t=6 index=222470 stride=1048576\n",
            "b=869 t=7 index=222471 stride=1048576\n",
            "b=879 t=0 index=225024 stride=1048576\n",
            "b=879 t=1 index=225025 stride=1048576\n",
            "b=879 t=2 index=225026 stride=1048576\n",
            "b=879 t=3 index=225027 stride=1048576\n",
            "b=879 t=4 index=225028 stride=1048576\n",
            "b=879 t=5 index=225029 stride=1048576\n",
            "b=879 t=6 index=225030 stride=1048576\n",
            "b=879 t=7 index=225031 stride=1048576\n",
            "b=854 t=0 index=218624 stride=1048576\n",
            "b=854 t=1 index=218625 stride=1048576\n",
            "b=854 t=2 index=218626 stride=1048576\n",
            "b=854 t=3 index=218627 stride=1048576\n",
            "b=854 t=4 index=218628 stride=1048576\n",
            "b=854 t=5 index=218629 stride=1048576\n",
            "b=854 t=6 index=218630 stride=1048576\n",
            "b=854 t=7 index=218631 stride=1048576\n",
            "b=868 t=0 index=222208 stride=1048576\n",
            "b=868 t=1 index=222209 stride=1048576\n",
            "b=868 t=2 index=222210 stride=1048576\n",
            "b=868 t=3 index=222211 stride=1048576\n",
            "b=868 t=4 index=222212 stride=1048576\n",
            "b=868 t=5 index=222213 stride=1048576\n",
            "b=868 t=6 index=222214 stride=1048576\n",
            "b=868 t=7 index=222215 stride=1048576\n",
            "b=857 t=0 index=219392 stride=1048576\n",
            "b=857 t=1 index=219393 stride=1048576\n",
            "b=857 t=2 index=219394 stride=1048576\n",
            "b=857 t=3 index=219395 stride=1048576\n",
            "b=857 t=4 index=219396 stride=1048576\n",
            "b=857 t=5 index=219397 stride=1048576\n",
            "b=857 t=6 index=219398 stride=1048576\n",
            "b=857 t=7 index=219399 stride=1048576\n",
            "b=855 t=0 index=218880 stride=1048576\n",
            "b=855 t=1 index=218881 stride=1048576\n",
            "b=855 t=2 index=218882 stride=1048576\n",
            "b=855 t=3 index=218883 stride=1048576\n",
            "b=855 t=4 index=218884 stride=1048576\n",
            "b=855 t=5 index=218885 stride=1048576\n",
            "b=855 t=6 index=218886 stride=1048576\n",
            "b=855 t=7 index=218887 stride=1048576\n",
            "b=860 t=0 index=220160 stride=1048576\n",
            "b=860 t=1 index=220161 stride=1048576\n",
            "b=860 t=2 index=220162 stride=1048576\n",
            "b=860 t=3 index=220163 stride=1048576\n",
            "b=860 t=4 index=220164 stride=1048576\n",
            "b=860 t=5 index=220165 stride=1048576\n",
            "b=860 t=6 index=220166 stride=1048576\n",
            "b=860 t=7 index=220167 stride=1048576\n",
            "b=858 t=0 index=219648 stride=1048576\n",
            "b=858 t=1 index=219649 stride=1048576\n",
            "b=858 t=2 index=219650 stride=1048576\n",
            "b=858 t=3 index=219651 stride=1048576\n",
            "b=858 t=4 index=219652 stride=1048576\n",
            "b=4073 t=5 index=1042693 stride=1048576\n",
            "b=4073 t=6 index=1042694 stride=1048576\n",
            "b=4073 t=7 index=1042695 stride=1048576\n",
            "b=4086 t=0 index=1046016 stride=1048576\n",
            "b=4086 t=1 index=1046017 stride=1048576\n",
            "b=4086 t=2 index=1046018 stride=1048576\n",
            "b=4086 t=3 index=1046019 stride=1048576\n",
            "b=4086 t=4 index=1046020 stride=1048576\n",
            "b=4086 t=5 index=1046021 stride=1048576\n",
            "b=4086 t=6 index=1046022 stride=1048576\n",
            "b=4086 t=7 index=1046023 stride=1048576\n",
            "b=4082 t=0 index=1044992 stride=1048576\n",
            "b=4082 t=1 index=1044993 stride=1048576\n",
            "b=4082 t=2 index=1044994 stride=1048576\n",
            "b=4082 t=3 index=1044995 stride=1048576\n",
            "b=4082 t=4 index=1044996 stride=1048576\n",
            "b=4082 t=5 index=1044997 stride=1048576\n",
            "b=4082 t=6 index=1044998 stride=1048576\n",
            "b=4082 t=7 index=1044999 stride=1048576\n",
            "b=4061 t=0 index=1039616 stride=1048576\n",
            "b=4061 t=1 index=1039617 stride=1048576\n",
            "b=4061 t=2 index=1039618 stride=1048576\n",
            "b=4061 t=3 index=1039619 stride=1048576\n",
            "b=4061 t=4 index=1039620 stride=1048576\n",
            "b=4061 t=5 index=1039621 stride=1048576\n",
            "b=4061 t=6 index=1039622 stride=1048576\n",
            "b=4061 t=7 index=1039623 stride=1048576\n",
            "b=4012 t=0 index=1027072 stride=1048576\n",
            "b=4012 t=1 index=1027073 stride=1048576\n",
            "b=4012 t=2 index=1027074 stride=1048576\n",
            "b=4012 t=3 index=1027075 stride=1048576\n",
            "b=4012 t=4 index=1027076 stride=1048576\n",
            "b=4012 t=5 index=1027077 stride=1048576\n",
            "b=4012 t=6 index=1027078 stride=1048576\n",
            "b=4012 t=7 index=1027079 stride=1048576\n",
            "b=4025 t=0 index=1030400 stride=1048576\n",
            "b=4025 t=1 index=1030401 stride=1048576\n",
            "b=4025 t=2 index=1030402 stride=1048576\n",
            "b=4025 t=3 index=1030403 stride=1048576\n",
            "b=4025 t=4 index=1030404 stride=1048576\n",
            "b=4025 t=5 index=1030405 stride=1048576\n",
            "b=4025 t=6 index=1030406 stride=1048576\n",
            "b=4025 t=7 index=1030407 stride=1048576\n",
            "b=4021 t=0 index=1029376 stride=1048576\n",
            "b=4021 t=1 index=1029377 stride=1048576\n",
            "b=4021 t=2 index=1029378 stride=1048576\n",
            "b=4021 t=3 index=1029379 stride=1048576\n",
            "b=4021 t=4 index=1029380 stride=1048576\n",
            "b=4021 t=5 index=1029381 stride=1048576\n",
            "b=4021 t=6 index=1029382 stride=1048576\n",
            "b=4021 t=7 index=1029383 stride=1048576\n",
            "b=4083 t=0 index=1045248 stride=1048576\n",
            "b=4083 t=1 index=1045249 stride=1048576\n",
            "b=4083 t=2 index=1045250 stride=1048576\n",
            "b=4083 t=3 index=1045251 stride=1048576\n",
            "b=4083 t=4 index=1045252 stride=1048576\n",
            "b=4083 t=5 index=1045253 stride=1048576\n",
            "b=4083 t=6 index=1045254 stride=1048576\n",
            "b=4083 t=7 index=1045255 stride=1048576\n",
            "b=4063 t=0 index=1040128 stride=1048576\n",
            "b=4063 t=1 index=1040129 stride=1048576\n",
            "b=4063 t=2 index=1040130 stride=1048576\n",
            "b=4063 t=3 index=1040131 stride=1048576\n",
            "b=4063 t=4 index=1040132 stride=1048576\n",
            "b=4063 t=5 index=1040133 stride=1048576\n",
            "b=4063 t=6 index=1040134 stride=1048576\n",
            "b=4063 t=7 index=1040135 stride=1048576\n",
            "b=4019 t=0 index=1028864 stride=1048576\n",
            "b=4019 t=1 index=1028865 stride=1048576\n",
            "b=4019 t=2 index=1028866 stride=1048576\n",
            "b=4019 t=3 index=1028867 stride=1048576\n",
            "b=4019 t=4 index=1028868 stride=1048576\n",
            "b=4019 t=5 index=1028869 stride=1048576\n",
            "b=4019 t=6 index=1028870 stride=1048576\n",
            "b=4019 t=7 index=1028871 stride=1048576\n",
            "b=4046 t=0 index=1035776 stride=1048576\n",
            "b=4046 t=1 index=1035777 stride=1048576\n",
            "b=4046 t=2 index=1035778 stride=1048576\n",
            "b=4046 t=3 index=1035779 stride=1048576\n",
            "b=4046 t=4 index=1035780 stride=1048576\n",
            "b=4046 t=5 index=1035781 stride=1048576\n",
            "b=4046 t=6 index=1035782 stride=1048576\n",
            "b=4046 t=7 index=1035783 stride=1048576\n",
            "b=4039 t=0 index=1033984 stride=1048576\n",
            "b=4039 t=1 index=1033985 stride=1048576\n",
            "b=4039 t=2 index=1033986 stride=1048576\n",
            "b=4039 t=3 index=1033987 stride=1048576\n",
            "b=4039 t=4 index=1033988 stride=1048576\n",
            "b=4039 t=5 index=1033989 stride=1048576\n",
            "b=4039 t=6 index=1033990 stride=1048576\n",
            "b=4039 t=7 index=1033991 stride=1048576\n",
            "b=4015 t=0 index=1027840 stride=1048576\n",
            "b=4015 t=1 index=1027841 stride=1048576\n",
            "b=4015 t=2 index=1027842 stride=1048576\n",
            "b=4015 t=3 index=1027843 stride=1048576\n",
            "b=4015 t=4 index=1027844 stride=1048576\n",
            "b=4015 t=5 index=1027845 stride=1048576\n",
            "b=4015 t=6 index=1027846 stride=1048576\n",
            "b=4015 t=7 index=1027847 stride=1048576\n",
            "b=4013 t=0 index=1027328 stride=1048576\n",
            "b=4013 t=1 index=1027329 stride=1048576\n",
            "b=4013 t=2 index=1027330 stride=1048576\n",
            "b=4013 t=3 index=1027331 stride=1048576\n",
            "b=4013 t=4 index=1027332 stride=1048576\n",
            "b=4013 t=5 index=1027333 stride=1048576\n",
            "b=4013 t=6 index=1027334 stride=1048576\n",
            "b=4013 t=7 index=1027335 stride=1048576\n",
            "b=4026 t=0 index=1030656 stride=1048576\n",
            "b=4026 t=1 index=1030657 stride=1048576\n",
            "b=4026 t=2 index=1030658 stride=1048576\n",
            "b=4026 t=3 index=1030659 stride=1048576\n",
            "b=4026 t=4 index=1030660 stride=1048576\n",
            "b=4026 t=5 index=1030661 stride=1048576\n",
            "b=4026 t=6 index=1030662 stride=1048576\n",
            "b=4026 t=7 index=1030663 stride=1048576\n",
            "b=4048 t=0 index=1036288 stride=1048576\n",
            "b=4048 t=1 index=1036289 stride=1048576\n",
            "b=4048 t=2 index=1036290 stride=1048576\n",
            "b=4048 t=3 index=1036291 stride=1048576\n",
            "b=4048 t=4 index=1036292 stride=1048576\n",
            "b=4048 t=5 index=1036293 stride=1048576\n",
            "b=4048 t=6 index=1036294 stride=1048576\n",
            "b=4048 t=7 index=1036295 stride=1048576\n",
            "b=4084 t=0 index=1045504 stride=1048576\n",
            "b=4084 t=1 index=1045505 stride=1048576\n",
            "b=4084 t=2 index=1045506 stride=1048576\n",
            "b=4084 t=3 index=1045507 stride=1048576\n",
            "b=4084 t=4 index=1045508 stride=1048576\n",
            "b=4084 t=5 index=1045509 stride=1048576\n",
            "b=4084 t=6 index=1045510 stride=1048576\n",
            "b=4084 t=7 index=1045511 stride=1048576\n",
            "b=4077 t=0 index=1043712 stride=1048576\n",
            "b=4077 t=1 index=1043713 stride=1048576\n",
            "b=4077 t=2 index=1043714 stride=1048576\n",
            "b=4077 t=3 index=1043715 stride=1048576\n",
            "b=4077 t=4 index=1043716 stride=1048576\n",
            "b=4077 t=5 index=1043717 stride=1048576\n",
            "b=4077 t=6 index=1043718 stride=1048576\n",
            "b=4077 t=7 index=1043719 stride=1048576\n",
            "b=4091 t=0 index=1047296 stride=1048576\n",
            "b=4091 t=1 index=1047297 stride=1048576\n",
            "b=4091 t=2 index=1047298 stride=1048576\n",
            "b=4091 t=3 index=1047299 stride=1048576\n",
            "b=4091 t=4 index=1047300 stride=1048576\n",
            "b=4091 t=5 index=1047301 stride=1048576\n",
            "b=4091 t=6 index=1047302 stride=1048576\n",
            "b=4091 t=7 index=1047303 stride=1048576\n",
            "b=4076 t=0 index=1043456 stride=1048576\n",
            "b=4076 t=1 index=1043457 stride=1048576\n",
            "b=4076 t=2 index=1043458 stride=1048576\n",
            "b=4076 t=3 index=1043459 stride=1048576\n",
            "b=4076 t=4 index=1043460 stride=1048576\n",
            "b=4076 t=5 index=1043461 stride=1048576\n",
            "b=4076 t=6 index=1043462 stride=1048576\n",
            "b=4076 t=7 index=1043463 stride=1048576\n",
            "b=4010 t=0 index=1026560 stride=1048576\n",
            "b=4010 t=1 index=1026561 stride=1048576\n",
            "b=4010 t=2 index=1026562 stride=1048576\n",
            "b=4010 t=3 index=1026563 stride=1048576\n",
            "b=4010 t=4 index=1026564 stride=1048576\n",
            "b=4010 t=5 index=1026565 stride=1048576\n",
            "b=4010 t=6 index=1026566 stride=1048576\n",
            "b=4010 t=7 index=1026567 stride=1048576\n",
            "b=4085 t=0 index=1045760 stride=1048576\n",
            "b=4085 t=1 index=1045761 stride=1048576\n",
            "b=4085 t=2 index=1045762 stride=1048576\n",
            "b=4085 t=3 index=1045763 stride=1048576\n",
            "b=4085 t=4 index=1045764 stride=1048576\n",
            "b=4085 t=5 index=1045765 stride=1048576\n",
            "b=4085 t=6 index=1045766 stride=1048576\n",
            "b=4085 t=7 index=1045767 stride=1048576\n",
            "b=4052 t=0 index=1037312 stride=1048576\n",
            "b=4052 t=1 index=1037313 stride=1048576\n",
            "b=4052 t=2 index=1037314 stride=1048576\n",
            "b=4052 t=3 index=1037315 stride=1048576\n",
            "b=4052 t=4 index=1037316 stride=1048576\n",
            "b=4052 t=5 index=1037317 stride=1048576\n",
            "b=4052 t=6 index=1037318 stride=1048576\n",
            "b=4052 t=7 index=1037319 stride=1048576\n",
            "b=4032 t=0 index=1032192 stride=1048576\n",
            "b=4032 t=1 index=1032193 stride=1048576\n",
            "b=4032 t=2 index=1032194 stride=1048576\n",
            "b=4032 t=3 index=1032195 stride=1048576\n",
            "b=4032 t=4 index=1032196 stride=1048576\n",
            "b=4032 t=5 index=1032197 stride=1048576\n",
            "b=4032 t=6 index=1032198 stride=1048576\n",
            "b=4032 t=7 index=1032199 stride=1048576\n",
            "b=4060 t=0 index=1039360 stride=1048576\n",
            "b=4060 t=1 index=1039361 stride=1048576\n",
            "b=4060 t=2 index=1039362 stride=1048576\n",
            "b=4060 t=3 index=1039363 stride=1048576\n",
            "b=4060 t=4 index=1039364 stride=1048576\n",
            "b=4060 t=5 index=1039365 stride=1048576\n",
            "b=4060 t=6 index=1039366 stride=1048576\n",
            "b=4060 t=7 index=1039367 stride=1048576\n",
            "b=4094 t=0 index=1048064 stride=1048576\n",
            "b=4094 t=1 index=1048065 stride=1048576\n",
            "b=4094 t=2 index=1048066 stride=1048576\n",
            "b=4094 t=3 index=1048067 stride=1048576\n",
            "b=4094 t=4 index=1048068 stride=1048576\n",
            "b=4094 t=5 index=1048069 stride=1048576\n",
            "b=4094 t=6 index=1048070 stride=1048576\n",
            "b=4094 t=7 index=1048071 stride=1048576\n",
            "b=4066 t=0 index=1040896 stride=1048576\n",
            "b=4066 t=1 index=1040897 stride=1048576\n",
            "b=4066 t=2 index=1040898 stride=1048576\n",
            "b=4066 t=3 index=1040899 stride=1048576\n",
            "b=4066 t=4 index=1040900 stride=1048576\n",
            "b=4066 t=5 index=1040901 stride=1048576\n",
            "b=4066 t=6 index=1040902 stride=1048576\n",
            "b=4066 t=7 index=1040903 stride=1048576\n",
            "b=4047 t=0 index=1036032 stride=1048576\n",
            "b=4047 t=1 index=1036033 stride=1048576\n",
            "b=4047 t=2 index=1036034 stride=1048576\n",
            "b=4047 t=3 index=1036035 stride=1048576\n",
            "b=4047 t=4 index=1036036 stride=1048576\n",
            "b=4047 t=5 index=1036037 stride=1048576\n",
            "b=4047 t=6 index=1036038 stride=1048576\n",
            "b=4047 t=7 index=1036039 stride=1048576\n",
            "b=4068 t=0 index=1041408 stride=1048576\n",
            "b=4068 t=1 index=1041409 stride=1048576\n",
            "b=4068 t=2 index=1041410 stride=1048576\n",
            "b=4068 t=3 index=1041411 stride=1048576\n",
            "b=4068 t=4 index=1041412 stride=1048576\n",
            "b=4068 t=5 index=1041413 stride=1048576\n",
            "b=4068 t=6 index=1041414 stride=1048576\n",
            "b=4068 t=7 index=1041415 stride=1048576\n",
            "b=4078 t=0 index=1043968 stride=1048576\n",
            "b=4078 t=1 index=1043969 stride=1048576\n",
            "b=4078 t=2 index=1043970 stride=1048576\n",
            "b=4078 t=3 index=1043971 stride=1048576\n",
            "b=4078 t=4 index=1043972 stride=1048576\n",
            "b=4078 t=5 index=1043973 stride=1048576\n",
            "b=4078 t=6 index=1043974 stride=1048576\n",
            "b=4078 t=7 index=1043975 stride=1048576\n",
            "b=4071 t=0 index=1042176 stride=1048576\n",
            "b=4071 t=1 index=1042177 stride=1048576\n",
            "b=4071 t=2 index=1042178 stride=1048576\n",
            "b=4071 t=3 index=1042179 stride=1048576\n",
            "b=4071 t=4 index=1042180 stride=1048576\n",
            "b=4071 t=5 index=1042181 stride=1048576\n",
            "b=4071 t=6 index=1042182 stride=1048576\n",
            "b=4071 t=7 index=1042183 stride=1048576\n",
            "b=4090 t=0 index=1047040 stride=1048576\n",
            "b=4090 t=1 index=1047041 stride=1048576\n",
            "b=4090 t=2 index=1047042 stride=1048576\n",
            "b=4090 t=3 index=1047043 stride=1048576\n",
            "b=4090 t=4 index=1047044 stride=1048576\n",
            "b=4090 t=5 index=1047045 stride=1048576\n",
            "b=4090 t=6 index=1047046 stride=1048576\n",
            "b=4090 t=7 index=1047047 stride=1048576\n",
            "b=4065 t=0 index=1040640 stride=1048576\n",
            "b=4065 t=1 index=1040641 stride=1048576\n",
            "b=4065 t=2 index=1040642 stride=1048576\n",
            "b=4065 t=3 index=1040643 stride=1048576\n",
            "b=4065 t=4 index=1040644 stride=1048576\n",
            "b=4065 t=5 index=1040645 stride=1048576\n",
            "b=4065 t=6 index=1040646 stride=1048576\n",
            "b=4065 t=7 index=1040647 stride=1048576\n",
            "b=4042 t=0 index=1034752 stride=1048576\n",
            "b=4042 t=1 index=1034753 stride=1048576\n",
            "b=4042 t=2 index=1034754 stride=1048576\n",
            "b=4042 t=3 index=1034755 stride=1048576\n",
            "b=4042 t=4 index=1034756 stride=1048576\n",
            "b=4042 t=5 index=1034757 stride=1048576\n",
            "b=4042 t=6 index=1034758 stride=1048576\n",
            "b=4042 t=7 index=1034759 stride=1048576\n",
            "b=4062 t=0 index=1039872 stride=1048576\n",
            "b=4062 t=1 index=1039873 stride=1048576\n",
            "b=4062 t=2 index=1039874 stride=1048576\n",
            "b=4062 t=3 index=1039875 stride=1048576\n",
            "b=4062 t=4 index=1039876 stride=1048576\n",
            "b=4062 t=5 index=1039877 stride=1048576\n",
            "b=4062 t=6 index=1039878 stride=1048576\n",
            "b=4062 t=7 index=1039879 stride=1048576\n",
            "b=4095 t=0 index=1048320 stride=1048576\n",
            "b=4095 t=1 index=1048321 stride=1048576\n",
            "b=4095 t=2 index=1048322 stride=1048576\n",
            "b=4095 t=3 index=1048323 stride=1048576\n",
            "b=4095 t=4 index=1048324 stride=1048576\n",
            "b=4095 t=5 index=1048325 stride=1048576\n",
            "b=4095 t=6 index=1048326 stride=1048576\n",
            "b=4095 t=7 index=1048327 stride=1048576\n",
            "b=4092 t=0 index=1047552 stride=1048576\n",
            "b=4092 t=1 index=1047553 stride=1048576\n",
            "b=4092 t=2 index=1047554 stride=1048576\n",
            "b=4092 t=3 index=1047555 stride=1048576\n",
            "b=4092 t=4 index=1047556 stride=1048576\n",
            "b=4092 t=5 index=1047557 stride=1048576\n",
            "b=4092 t=6 index=1047558 stride=1048576\n",
            "b=4092 t=7 index=1047559 stride=1048576\n",
            "b=4079 t=0 index=1044224 stride=1048576\n",
            "b=4079 t=1 index=1044225 stride=1048576\n",
            "b=4079 t=2 index=1044226 stride=1048576\n",
            "b=4079 t=3 index=1044227 stride=1048576\n",
            "b=4079 t=4 index=1044228 stride=1048576\n",
            "b=4079 t=5 index=1044229 stride=1048576\n",
            "b=4079 t=6 index=1044230 stride=1048576\n",
            "b=4079 t=7 index=1044231 stride=1048576\n",
            "Max error: 0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7Tz-xo3d1oX"
      },
      "source": [
        "That's another big speedup from running multiple blocks! (Note your results may vary from the blog post due to whatever GPU you've been allocated by Colab. If you notice your speedups for the final example are not as drastic as those in the blog post, check out #4 in the *Exercises* section below.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ja5CiQZpicHC"
      },
      "source": [
        "## Exercises"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BEijwk25id3t"
      },
      "source": [
        "To keep you going, here are a few things to try on your own.\n",
        "\n",
        "1. Browse the [CUDA Toolkit documentation](https://docs.nvidia.com/cuda/index.html). If you haven’t installed CUDA yet, check out the [Quick Start Guide](https://docs.nvidia.com/cuda/cuda-quick-start-guide/index.html) and the installation guides. Then browse the [Programming Guide](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html) and the [Best Practices Guide](https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html). There are also tuning guides for various architectures.\n",
        "2. Experiment with `printf()` inside the kernel. Try printing out the values of `threadIdx.x` and `blockIdx.x` for some or all of the threads. Do they print in sequential order? Why or why not?\n",
        "3. Print the value of `threadIdx.y` or `threadIdx.z` (or `blockIdx.y`) in the kernel. (Likewise for `blockDim` and `gridDim`). Why do these exist? How do you get them to take on values other than 0 (1 for the dims)?\n",
        "4. If you have access to a [Pascal-based GPU](https://developer.nvidia.com/blog/inside-pascal/), try running `add_grid.cu` on it. Is performance better or worse than the K80 results? Why? (Hint: read about [Pascal’s Page Migration Engine and the CUDA 8 Unified Memory API](https://developer.nvidia.com/blog/beyond-gpu-memory-limits-unified-memory-pascal/).) For a detailed answer to this question, see the post [Unified Memory for CUDA Beginners](https://developer.nvidia.com/blog/unified-memory-cuda-beginners/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NpWVnIPujp0K"
      },
      "source": [
        "## Where to From Here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nTyQePjlkRJ3"
      },
      "source": [
        "If you enjoyed this notebook and want to learn more, the [NVIDIA DLI](https://nvidia.com/dli) offers several in depth CUDA Programming courses.\n",
        "\n",
        "For those of you just starting out, please consider [_Fundamentals of Accelerated Computing with CUDA C/C++_](https://courses.nvidia.com/courses/course-v1:DLI+C-AC-01+V1/about) which provides dedicated GPU resources, a more sophisticated programming environment, use of the [NVIDIA Nsight Systems™](https://developer.nvidia.com/nsight-systems) visual profiler, dozens of interactive exercises, detailed presentations, over 8 hours of material, and the ability to earn a DLI Certificate of Competency.\n",
        "\n",
        "Similarly, for Python programmers, please consider [_Fundamentals of Accelerated Computing with CUDA Python_](https://courses.nvidia.com/courses/course-v1:DLI+C-AC-02+V1/about).\n",
        "\n",
        "For more intermediate and advance CUDA programming materials, please check out the _Accelerated Computing_ section of the NVIDIA DLI [self-paced catalog](https://www.nvidia.com/en-us/training/online/)."
      ]
    }
  ]
}